{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from dotenv import load_dotenv\n",
    "from together import TogetherClient\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Load TogetherAI API key from environment\n",
    "together_api_key = os.getenv('TOGETHER_API_KEY')\n",
    "if not together_api_key:\n",
    "    raise ValueError(\"TOGETHER_API_KEY is not set in the environment.\")\n",
    "\n",
    "# Initialize TogetherClient\n",
    "try:\n",
    "    client = TogetherClient(api_key=together_api_key)\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing TogetherClient: {e}\")\n",
    "    raise\n",
    "\n",
    "# Define the models you want to use\n",
    "model_names = [\n",
    "    \"together-gpt-4\",        # GPT-4\n",
    "    \"together-mistral\",      # Mistral\n",
    "    \"together-llama\",        # LLaMA\n",
    "    \"together-gpt-3.5-turbo\",# GPT-3.5-turbo\n",
    "    \"together-gpt-neo\"       # GPT-Neo\n",
    "]\n",
    "\n",
    "# Create templates for each LLM\n",
    "templates = [\n",
    "    PromptTemplate(\n",
    "        input_variables=[\"input\"],\n",
    "        template=\"You are in a debate. You will be speaking FOR the topic. Respond to: {input}\"\n",
    "    ),\n",
    "    PromptTemplate(\n",
    "        input_variables=[\"input\"],\n",
    "        template=\"You are in a debate. You will be speaking AGAINST the topic. Respond to: {input}\"\n",
    "    ),\n",
    "    PromptTemplate(\n",
    "        input_variables=[\"input\"],\n",
    "        template=\"You are in a debate. You will be speaking FOR the topic. Respond to: {input}\"\n",
    "    ),\n",
    "    PromptTemplate(\n",
    "        input_variables=[\"input\"],\n",
    "        template=\"You are in a debate. You will be speaking AGAINST the topic. Respond to: {input}\"\n",
    "    ),\n",
    "    PromptTemplate(\n",
    "        input_variables=[\"input\"],\n",
    "        template=\"You are in a debate. You will be speaking FOR the topic. Respond to: {input}\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# Create closing templates for each LLM\n",
    "closing_templates = [\n",
    "    PromptTemplate(\n",
    "        input_variables=[\"input\"],\n",
    "        template=\"You are in a debate. You will be speaking FOR the topic. Provide your closing arguments based on the debate: {input}\"\n",
    "    ),\n",
    "    PromptTemplate(\n",
    "        input_variables=[\"input\"],\n",
    "        template=\"You are in a debate. You will be speaking AGAINST the topic. Provide your closing arguments based on the debate: {input}\"\n",
    "    ),\n",
    "    PromptTemplate(\n",
    "        input_variables=[\"input\"],\n",
    "        template=\"You are in a debate. You will be speaking FOR the topic. Provide your closing arguments based on the debate: {input}\"\n",
    "    ),\n",
    "    PromptTemplate(\n",
    "        input_variables=[\"input\"],\n",
    "        template=\"You are in a debate. You will be speaking AGAINST the topic. Provide your closing arguments based on the debate: {input}\"\n",
    "    ),\n",
    "    PromptTemplate(\n",
    "        input_variables=[\"input\"],\n",
    "        template=\"You are in a debate. You will be speaking FOR the topic. Provide your closing arguments based on the debate: {input}\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# Create LLMs and chains for each LLM\n",
    "llms = [client.create_llm(model_name=model_name) for model_name in model_names]\n",
    "chains = [LLMChain(llm=llms[i], prompt=templates[i]) for i in range(5)]\n",
    "closing_chains = [LLMChain(llm=llms[i], prompt=closing_templates[i]) for i in range(5)]\n",
    "\n",
    "# Start the debate\n",
    "input_text = input(\"Enter the initial debate topic: \")\n",
    "max_rounds = 10\n",
    "round_counter = 0\n",
    "\n",
    "while round_counter < max_rounds:\n",
    "    for i in range(5):\n",
    "        response = chains[i].invoke({\"input\": input_text})\n",
    "        response_text = response['text']\n",
    "        print(f\"Debater {chr(65 + i)}: {response_text}\")\n",
    "        input_text = response_text\n",
    "        round_counter += 1\n",
    "        if round_counter >= max_rounds:\n",
    "            break\n",
    "\n",
    "    if round_counter % 5 == 0 and round_counter < max_rounds:\n",
    "        input_text = input(\"Enter a new debate topic or continue the current one (or type 'exit' to quit): \")\n",
    "        if input_text.lower() == 'exit':\n",
    "            break\n",
    "\n",
    "# Closing arguments\n",
    "closing_input = input_text\n",
    "closing_responses = [closing_chains[i].invoke({\"input\": closing_input}) for i in range(5)]\n",
    "\n",
    "print(\"\\nClosing Arguments:\")\n",
    "for i in range(5):\n",
    "    print(f\"Debater {chr(65 + i)}: {closing_responses[i]['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from together import Together\n",
    "\n",
    "client = Together(api_key=os.environ.get('TOGETHER_API_KEY'))\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
    "    messages=[],\n",
    "    max_tokens=512,\n",
    "    temperature=0.7,\n",
    "    top_p=0.7,\n",
    "    top_k=50,\n",
    "    repetition_penalty=1,\n",
    "    stop=[\"<|eot_id|>\"],\n",
    "    stream=True\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
